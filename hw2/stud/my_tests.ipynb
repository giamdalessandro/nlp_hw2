{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('nlp_hw2': venv)"
  },
  "interpreter": {
   "hash": "35bf5652c149c8ef1d689001f7b21f133df127512c57592ea276e1e13d983d75"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# POS tagging"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "import os\n",
    "import nltk\n",
    "\n",
    "if not os.path.isdir(\"../../data/nltk/\"):\n",
    "\t# check whether nltk data are already downloaded\n",
    "    nltk.download(\"averaged_perceptron_tagger\", download_dir=\"../../data/nltk/\")  # textblob\n",
    "    nltk.download(\"subjectivity\", download_dir=\"../../data/nltk/\")                # nltk.subjectivity\n",
    "# to load from file\n",
    "nltk.data.path.append(\"../../data/nltk/\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## textBlob\n",
    "POS-tagging via textblob"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "text = '''The titular threat of The Blob has always struck me as the ultimate movie\n",
    "monster: an insatiably hungry, amoeba-like mass able to penetrate\n",
    "virtually any safeguard, capable of--as a doomed doctor chillingly\n",
    "describes it--\"assimilating flesh on contact.\n",
    "Snide comparisons to gelatin be damned, it's a concept with the most\n",
    "devastating of potential consequences, not unlike the grey goo scenario\n",
    "proposed by technological theorists fearful of\n",
    "artificial intelligence run rampant.'''\n",
    "\n",
    "# ortography changes resulting pos_tags \n",
    "text2 = \"the movie begins in the past where a boy named sam attempts to save celebi from a hunter.\"\n",
    "\n",
    "blob = TextBlob(text2)\n",
    "\n",
    "# to get pos_tags\n",
    "blob.tags\n",
    "# to get noun phrases\n",
    "#blob.noun_phrases"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## nltk\n",
    "POS-tagging via nltk"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "\n",
    "n_instances = 100\n",
    "subj_docs = [(sent, 'subj') for sent in subjectivity.sents(categories='subj')[:n_instances]]\n",
    "obj_docs  = [(sent, 'obj') for sent in subjectivity.sents(categories='obj')[:n_instances]]\n",
    "\n",
    "#subj_docs[0], obj_docs[0]\n",
    "#len(subj_docs), len(obj_docs)\n",
    "\n",
    "tags = nltk.pos_tag(obj_docs[0][0])\n",
    "tags\n",
    "\n",
    "text2 = \"the movie begins in the past where a boy named sam attempts to save celebi from a hunter.\"\n",
    "nltk.pos_tag(text2.strip().split(\" \"))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('the', 'DT'),\n",
       " ('movie', 'NN'),\n",
       " ('begins', 'VBZ'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('past', 'NN'),\n",
       " ('where', 'WRB'),\n",
       " ('a', 'DT'),\n",
       " ('boy', 'NN'),\n",
       " ('named', 'VBN'),\n",
       " ('sam', 'JJ'),\n",
       " ('attempts', 'NNS'),\n",
       " ('to', 'TO'),\n",
       " ('save', 'VB'),\n",
       " ('celebi', 'NN'),\n",
       " ('from', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('hunter.', 'NN')]"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Named-Entity Recognition\n",
    "Vocab and pre-trained embeddings tests"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## pre-trained embeddings"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import torchtext\n",
    "\n",
    "vec = torchtext.vocab.GloVe(name='6B', dim=50)\n",
    "\n",
    "tokens = ['<UNK>', 'zio']\n",
    "vec.get_vecs_by_tokens(tokens, lower_case_backup=True)\n",
    "\n",
    "#vec.vectors.size()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.4264, -0.2981,  0.3472,  0.5420, -0.1408, -0.3406,  0.8830, -0.8148,\n",
       "         -0.2890, -0.0779,  0.0931,  0.0395, -0.0308, -0.0490,  0.1947, -0.7215,\n",
       "         -0.8993,  0.6439,  0.7098,  0.5537, -1.3525, -0.6833,  0.0090,  0.3511,\n",
       "          0.5957,  0.8872, -0.4650,  0.1624, -0.4045, -0.4708, -0.8202,  0.0468,\n",
       "          0.2596,  0.9540, -0.2491,  0.2000,  0.5608,  0.0319, -0.9115,  0.7136,\n",
       "          0.3106,  0.0678, -0.2949, -0.2032, -0.1507, -0.2053,  0.4046, -0.3827,\n",
       "         -0.2589,  0.5037]])"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Transformers\n",
    "\n",
    "### Need to choose correct BERT pre-trained model for the task, for every model the output size, and meaning, changes.\n",
    "- **BertSequenceClassification**: batch x seq_len x 2 \n",
    "- **BertTokenClassification**: batch x seq_len x 2\n",
    "- **BertModel**: batch x seq_len x hidden_size(768) "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "from transformers import BertTokenizer, BertModel, \\\n",
    "\t\tBertForTokenClassification, BertForSequenceClassification\n",
    "  \n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "\t\"bert-base-cased\",\n",
    "\tnum_labels=2)\n",
    "\n",
    "x = [\n",
    "\t[\"Each element in list of batch should be of equal size.\",\"cacca\"],\n",
    "\t(\"Penguins jumps with each other, just to see if the water's still cold.\", \"daino\"),\n",
    "\t(\"This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\", \"vvv\")\n",
    "]\n",
    "\n",
    "inp = tokenizer.encode(x[0][0], x[0][1])\n",
    "#nn = tokenizer.tokenize(x)\n",
    "print(inp)\n",
    "\n",
    "# out = model(**inp)\n",
    "\n",
    "# out.last_hidden_state.size()   # if model BertModel\n",
    "# out               # if model BertFor...Classification"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[101, 100, 100, 102]\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}