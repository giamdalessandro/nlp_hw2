{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('nlp_hw2': venv)"
  },
  "interpreter": {
   "hash": "35bf5652c149c8ef1d689001f7b21f133df127512c57592ea276e1e13d983d75"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# POS tagging"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "import os\n",
    "import nltk\n",
    "\n",
    "if not os.path.isdir(\"../../data/nltk/\"):\n",
    "\t# check whether nltk data are already downloaded\n",
    "    nltk.download(\"averaged_perceptron_tagger\", download_dir=\"../../data/nltk/\")  # textblob\n",
    "    nltk.download(\"subjectivity\", download_dir=\"../../data/nltk/\")                # nltk.subjectivity\n",
    "# to load from file\n",
    "nltk.data.path.append(\"../../data/nltk/\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## textBlob\n",
    "POS-tagging via textblob"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "text = '''The titular threat of The Blob has always struck me as the ultimate movie\n",
    "monster: an insatiably hungry, amoeba-like mass able to penetrate\n",
    "virtually any safeguard, capable of--as a doomed doctor chillingly\n",
    "describes it--\"assimilating flesh on contact.\n",
    "Snide comparisons to gelatin be damned, it's a concept with the most\n",
    "devastating of potential consequences, not unlike the grey goo scenario\n",
    "proposed by technological theorists fearful of\n",
    "artificial intelligence run rampant.'''\n",
    "\n",
    "# ortography changes resulting pos_tags \n",
    "text2 = \"the movie begins in the past where a boy named sam attempts to save celebi from a hunter.\"\n",
    "\n",
    "blob = TextBlob(text2)\n",
    "\n",
    "# to get pos_tags\n",
    "blob.tags\n",
    "# to get noun phrases\n",
    "#blob.noun_phrases"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## nltk\n",
    "POS-tagging via nltk"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "\n",
    "n_instances = 100\n",
    "subj_docs = [(sent, 'subj') for sent in subjectivity.sents(categories='subj')[:n_instances]]\n",
    "obj_docs  = [(sent, 'obj') for sent in subjectivity.sents(categories='obj')[:n_instances]]\n",
    "\n",
    "#subj_docs[0], obj_docs[0]\n",
    "#len(subj_docs), len(obj_docs)\n",
    "\n",
    "tags = nltk.pos_tag(obj_docs[0][0])\n",
    "tags\n",
    "\n",
    "text2 = \"the movie begins in the past where a boy named sam attempts to save celebi from a hunter.\"\n",
    "nltk.pos_tag(text2.strip().split(\" \"))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('the', 'DT'),\n",
       " ('movie', 'NN'),\n",
       " ('begins', 'VBZ'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('past', 'NN'),\n",
       " ('where', 'WRB'),\n",
       " ('a', 'DT'),\n",
       " ('boy', 'NN'),\n",
       " ('named', 'VBN'),\n",
       " ('sam', 'JJ'),\n",
       " ('attempts', 'NNS'),\n",
       " ('to', 'TO'),\n",
       " ('save', 'VB'),\n",
       " ('celebi', 'NN'),\n",
       " ('from', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('hunter.', 'NN')]"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Named-Entity Recognition\n",
    "Vocab and pre-trained embeddings tests"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## pre-trained embeddings"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import torchtext\n",
    "\n",
    "vec = torchtext.vocab.GloVe(name='6B', dim=50)\n",
    "\n",
    "tokens = ['<UNK>', 'zio']\n",
    "vec.get_vecs_by_tokens(tokens, lower_case_backup=True)\n",
    "\n",
    "#vec.vectors.size()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.4264, -0.2981,  0.3472,  0.5420, -0.1408, -0.3406,  0.8830, -0.8148,\n",
       "         -0.2890, -0.0779,  0.0931,  0.0395, -0.0308, -0.0490,  0.1947, -0.7215,\n",
       "         -0.8993,  0.6439,  0.7098,  0.5537, -1.3525, -0.6833,  0.0090,  0.3511,\n",
       "          0.5957,  0.8872, -0.4650,  0.1624, -0.4045, -0.4708, -0.8202,  0.0468,\n",
       "          0.2596,  0.9540, -0.2491,  0.2000,  0.5608,  0.0319, -0.9115,  0.7136,\n",
       "          0.3106,  0.0678, -0.2949, -0.2032, -0.1507, -0.2053,  0.4046, -0.3827,\n",
       "         -0.2589,  0.5037]])"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Transformers\n",
    "\n",
    "### Need to choose correct BERT pre-trained model for the task, for every model the output size, and meaning, changes.\n",
    "- **BertSequenceClassification**: batch x seq_len x 2 \n",
    "- **BertTokenClassification**: batch x seq_len x 2\n",
    "- **BertModel**: batch x seq_len x hidden_size(768) "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "from torch import nn\n",
    "from transformers import BertTokenizer, BertModel, \\\n",
    "\t\tBertForTokenClassification, BertForSequenceClassification, \\\n",
    "\t\tRobertaForSequenceClassification, RobertaTokenizer\n",
    "  \n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "model = RobertaForSequenceClassification.from_pretrained(\n",
    "\t\"roberta-base\",\n",
    "\tnum_labels=5\n",
    ")\n",
    "## custom classifier head\n",
    "#classifier_head = nn.Sequential(\n",
    "#\tnn.Dropout(0.2),\n",
    "#\tnn.Linear(768, 64),\n",
    "#\tnn.GELU(), #nn.ReLU\n",
    "#\tnn.Linear(64, 5),\n",
    "#)\n",
    "#model.classifier = classifier_head\n",
    "\n",
    "\n",
    "x = [\"Each element in list of batch should be of equal size.\",\"cacca\"]\n",
    "cc = ['Seriously, this place kicks ass.', 'Not enough wines by the glass either.', 'My wife and I always enjoy the young, not always well trained but nevertheless friendly, staff, all of whom have a story.', 'One of the more authentic Shanghainese restaurants in the US definitely the best in Manhattan Chinatown.', 'Appetizers are somewhere around $7 each and the main dishes are between $11 and $16.', \"If you've ever been along the river in Weehawken you have an idea of the top of view the chart house has to offer.\", \"Why make a reservation if you aren't going to keep it?\", 'We took advanatage of the half price sushi deal on saturday so it was well worth it.', 'I was wrong.', 'Get the feeling they settled into a groove a while ago.', \"If you want to save some money, don't go here.\", 'and for this i had to shell out a small fortune?', \"I've been to Joya twice and I will never go again.\", 'The food was good.', 'i actually feel like i should keep it a secret.', 'i would just ask for no oil next time.', 'The food is delicious.', 'The staff was accomodating, the food was absolutely delicious and the place is lovely.', 'Definitely not a restaurant to skip!', \"We had a girls' night dinner here for restaurant week.\", 'Good food at the restaurant (a bit expensive, but great if you want to impress your date).', 'In fact, two people could really share one plate.', 'Highly recommended.', \"But this place is a well-oiled machine so they know what they're doing.\", 'All the desserts the group tried got favorable reviews.', 'I loved it and will be back soon.', 'My husband and I have been there a couple of times and each time we sat at the sushi bar (chef Yoshi) and ordered everything ala carte.', 'However, I think this place is a good hang out spot.', 'My wife and I were in NYC celebrating our 30th anniversary.', 'For myself and family we would only go here to celebrate an occassion, I do prefer smaller, quiter restaurants.', 'Not pretentious and very economical.', 'Other guests enjoyed pizza, santa fe chopped salad and fish and chips.']\n",
    "#\t(\"Penguins jumps with each other, just to see if the water's still cold.\", \"daino\"),\n",
    "#\t(\"This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical #   initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\", \"vvv\")]\n",
    "\n",
    "#inp = tokenizer(cc, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "inp = tokenizer.tokenize(cc[0])\n",
    "#nn = model(**inp)\n",
    "\n",
    "#print(nn.logits.size())\n",
    "print(inp)\n",
    "\n",
    "# out = model(**inp)\n",
    "# out.last_hidden_state.size()   # if model BertModel\n",
    "# out               # if model BertFor...Classification"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Seriously', ',', 'Ġthis', 'Ġplace', 'Ġkicks', 'Ġass', '.']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# COsastre"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "import torch\n",
    "\n",
    "sigmoid = torch.nn.Sigmoid() # .SiLU()\n",
    "threshold = torch.nn.Threshold(0.5,0)\n",
    "x = torch.Tensor([[2, -2.6, -0.6], \n",
    "\t\t\t\t  [1, 2, 3]])\n",
    "\n",
    "x = sigmoid(x)\n",
    "print(\"pre:\",x)\n",
    "\n",
    "t = threshold = torch.tensor([0.5])\n",
    "(x>t).float()\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "pre: tensor([[0.8808, 0.0691, 0.3543],\n",
      "        [0.7311, 0.8808, 0.9526]])\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "metadata": {}
  }
 ]
}